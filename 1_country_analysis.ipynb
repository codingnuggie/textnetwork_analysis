{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ce781",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# networkx has to be networkx-3.0\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import scipy\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt # !pip install matplotlib -U # 3.7.0\n",
    "import numpy as np\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad22226",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e080ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.read_pickle('processed_docs/loaded_txt_files.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.head(3) # Excerpt of the loaded documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ceccb",
   "metadata": {},
   "source": [
    "# 1. Select country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20077af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: .copy() To esure that the original df \"docs_df\" is not affected by\n",
    "# any further cleaning steps\n",
    "\n",
    "selection = docs_df.loc[(docs_df['country']=='us') & \n",
    "                       (docs_df['category']=='strategy')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a442e",
   "metadata": {},
   "source": [
    "# 2. Clean and prepare the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d87483",
   "metadata": {},
   "source": [
    "The custom_stop_words list below is iteratively filled based on preliminary results of each country (i.e., each country may have different stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6832bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['use', 'due', 'well', 'however', \n",
    "                     'within', 'must',\n",
    "                     'also', 'since', 'table', 'text',\n",
    "                     'finally', 'day', 'sometimes', 'issue',\n",
    "                     'section', 'set', 'used', 'belief', 'thus',\n",
    "                     'may', 'stated', 'system', 'forth', 'outlined',\n",
    "                     'including', 'three', 'example', 'some', 'ass',\n",
    "                     'step', 'take', 'call', 'whether', 'number', 'make',\n",
    "                     'much', 'shall', 'using', 'data', 'therefore', 'agency', 'yet',\n",
    "                     'date', 'title', 'subject', 'february', 'nearly', 'chief', 'officer',\n",
    "                     'secretary', 'head', 'director', 'year', 'annual', 'etc', 'new', 'many', 'little', \n",
    "                     'purely', 'would', 'will', 'last', 'today', 'often', 'past', 'already', 'put', \n",
    "                     'another', 'simply', 'without', 'widely', 'otherwise', 'one', 'moreover', 'better',\n",
    "                     'fully', 'could', 'can', 'should', 'upon', 'every','bring', 'written', 'recent', 'mean', 'fit', \n",
    "                     'although', 'seeing', 'fill', 'select', 'part', 'turn', 'might', 'likely', 'taken', \n",
    "                     'eighth', 'indeed', '1960s', 'five', 'six', 'second', 'annex', 'lastly', 'firstly', 'along', \n",
    "                     'million', 'going', 'head', '20year', 'futherance', 'third', 'subsection', \n",
    "                     'always', 'forgoing', 'orginally', 'see', 'team', 'forbearing', 'even', 'given', \n",
    "                     'making', 'among', 'two', 'unnecessarily', 'necessarily'\n",
    "                    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Depending on the size of the text and number of documents this might take a while\n",
    "# Note: The `prepare_text` function needs a list of text as input\n",
    "selection['prepared_text'] = selection['text'].apply(lambda x: prepare_text([x], custom_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f099e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the document's name as index\n",
    "selection.set_index('file', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt of the final data set\n",
    "selection.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d498f2",
   "metadata": {},
   "source": [
    "# 3. Create co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516ee93",
   "metadata": {},
   "source": [
    "* How often words occur together in a sentence. \n",
    "* The concept of term-context matrix is used, in which each sentence is represented as a context. If two terms (words) occur in the same context, they are said to have occured in the same occurence context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: When applying the `.values.tolist()` the result is too nested\n",
    "# list(itertools.chain.from_iterable()) takes care of that\n",
    "prepared_input = list(itertools.chain.from_iterable(selection['prepared_text'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get nodes and co-occurrence matrix from prepared text\n",
    "nodes, matrix = create_context_matrix(prepared_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of nodes: {len(nodes)} - Shape of co-occurrence matrix: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8325d22",
   "metadata": {},
   "source": [
    "# 4. Pre-cleaning to shrink the data before creating the graph\n",
    "\n",
    "1. remove some words from the matrix using tf-idf\n",
    "    \n",
    "2. remove some co-occurences (edges) from the matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd44d0b",
   "metadata": {},
   "source": [
    "### 4.1 Using tf-idf as a filter to reduce the no. of words by removing words that have lower importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TfidfVectorizer needs the (cleaned) text in a sentence format\n",
    "# Therefore, another column is created containing the prepared text as sentences\n",
    "def format_prepared_text(text):\n",
    "    \"\"\"\n",
    "    [[w1, w2], [wa, wb]] -> w1 w2. wa wb.\n",
    "    \"\"\"\n",
    "    y = [' '.join(i) for i in text]\n",
    "    return '. '.join(y)    \n",
    "\n",
    "selection['prepared_text_tfidf'] = selection['prepared_text'].apply(lambda x: format_prepared_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c63558",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize and apply the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(selection['prepared_text_tfidf'].values.tolist())\n",
    "\n",
    "# Create a DataFrame from the output\n",
    "tfidf_df = pd.DataFrame(X.toarray().transpose(),\n",
    "             columns=selection.index,\n",
    "             index=vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_df['sum'] = tfidf_df.sum(axis=1) # sum up each words' tf-idf scores across all documents to get total\n",
    "tfidf_df.sort_values(by=\"sum\", ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08010e0c",
   "metadata": {},
   "source": [
    "#### Define threshold to drop words: \n",
    "* `0.25` = the 25th percentile (this is chosen to be most appropriate)\n",
    "* `0.5` = median\n",
    "* `0.75` = the 75th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a92a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove_tfidf = tfidf_df.loc[tfidf_df['sum'] < tfidf_df['sum'].quantile(0.25)].index.to_list()\n",
    "len(words_to_remove_tfidf)\n",
    "\n",
    "print(f\"By applying this filter {len(words_to_remove_tfidf)} nodes would be dropped from {len(nodes)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60425da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_filtered_matrix = matrix.drop(index=words_to_remove_tfidf, \n",
    "                            columns=words_to_remove_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_filtered_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01eb2ad",
   "metadata": {},
   "source": [
    "### 4.2 Remove co-occurences (edges) between words that appear too little times to be considered important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_matrix = tfidf_filtered_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The edge_weights series has the same shape as nodes * nodes\n",
    "# So that every combination between each word is represented\n",
    "edge_weights = pd.Series(selected_matrix.to_numpy(copy=True).flatten()) # one-dimensional list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create elbow plot for edges\n",
    "alt = pd.DataFrame(edge_weights, columns=[\"edge_weight\"])\n",
    "alt = alt.reset_index()\n",
    "alt = alt[alt[\"edge_weight\"]>0]#0 = no connection\n",
    "alt = alt.groupby([\"edge_weight\"], as_index=False).count()\n",
    "alt.columns=[\"edge_weight\", \"edge_freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.plot(alt[\"edge_weight\"],alt[\"edge_freq\"])\n",
    "\n",
    "kl = KneeLocator(alt[\"edge_weight\"], alt[\"edge_freq\"], S=1, curve='convex', direction='decreasing')\n",
    "knee_point = alt[alt[\"edge_weight\"]==kl.knee]\n",
    "\n",
    "ax.plot(knee_point[\"edge_weight\"], knee_point[\"edge_freq\"],marker=\"o\")\n",
    "ax.annotate(knee_point[\"edge_weight\"].values[0], \n",
    "            (knee_point['edge_weight'].values[0], \n",
    "             knee_point['edge_freq'].values[0]))\n",
    "ax.set_ylabel(\"Edge Frequency\")\n",
    "ax.set_xlabel(\"Edge Weight\")\n",
    "ax.set_title(\"Edge Weight vs. Edge Frequency for the US\")\n",
    "\n",
    "current_values = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels(['{:,.0f}'.format(x) for x in current_values])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('elbow_US.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38df97",
   "metadata": {},
   "source": [
    "#### Define cut-off threshold and apply shrinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff16ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_threshold = 5\n",
    "shrinked_matrix = selected_matrix.mask(selected_matrix < edge_threshold).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0f522",
   "metadata": {},
   "source": [
    "# 5. Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "graph = create_nxgraph(shrinked_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43701f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there may be some isolated nodes (have no connection to other nodes)\n",
    "# hence drop them from the graph\n",
    "graph.remove_nodes_from(list(nx.isolates(graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result\n",
    "get_graph_attributes(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8b288",
   "metadata": {},
   "source": [
    "# 6. Create/identify communities via Leiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35368bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_com = create_community_graph(graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the identified clusterse or communities\n",
    "get_community_summary(graph_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top n words per community \n",
    "top_n_words_by_community(graph_com).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ba035",
   "metadata": {},
   "source": [
    "### 6.1 Select the communities large enough for visualization (e.g., at least 20 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da8e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nodes = [x for x,y in graph_com.nodes(data=True) if y['community'] in [0, 1, 2, 3, 4, 5, 6, 7, 8]]\n",
    "\n",
    "# Note: When creating a subgraph from a graph - a connection to the graph remains\n",
    "# Changes made on the subgraph apply on the graph. To avoid this use the following:\n",
    "# graph_com_top9 = graph_com.subgraph(selected_nodes) # connection would remain\n",
    "\n",
    "graph_com_top9 = graph_com.subgraph(selected_nodes).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c9038",
   "metadata": {},
   "source": [
    "### 6.2 Get some descriptive analysis about the top n words per community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c327280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words_by_community(graph_com_top9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b63c1d",
   "metadata": {},
   "source": [
    "# 7. Vizualizations \n",
    "vizualize one selected community each time via matplotlib or to be exported to Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph for each individual community\n",
    "com_one = graph_per_community(graph_com_top9, 0)\n",
    "com_two = graph_per_community(graph_com_top9, 1)\n",
    "com_three = graph_per_community(graph_com_top9, 2)\n",
    "com_four = graph_per_community(graph_com_top9, 3)\n",
    "com_five = graph_per_community(graph_com_top9, 4)\n",
    "com_six = graph_per_community(graph_com_top9, 5)\n",
    "com_seven = graph_per_community(graph_com_top9, 6)\n",
    "com_eight = graph_per_community(graph_com_top9, 7)\n",
    "com_nine = graph_per_community(graph_com_top9, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_community_graph(com_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_graph_attributes(com_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_to_gml(com_one, 'graph_com_one_US')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c8a92",
   "metadata": {},
   "source": [
    "End of Notebook "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
